{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64\\\\Banana.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dqn\"></a>\n",
    "***************\n",
    "#### 4-1. Definite a DQN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import random\n",
    "import collections\n",
    "import pickle\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, build_network, lr=0.001, discount_factor=.99,\n",
    "                 replay_mem_size=2500, batch_size=32, smooth_update_tau=0.001):\n",
    "        \n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        torch.cuda.set_device(0)\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = smooth_update_tau\n",
    "        self.discount_factor = discount_factor\n",
    "        self.replay_memory = collections.deque(maxlen=replay_mem_size)\n",
    "        self.runner_network = build_network.cuda()\n",
    "        self.target_network = build_network.cuda()\n",
    "\n",
    "        self.runner_optimizer = torch.optim.Adam(self.runner_network.parameters(), lr=self.lr)\n",
    "        \n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.runner_network.state_dict())\n",
    "    \n",
    "    def update_target_network_smooth(self):\n",
    "        # refer to https://github.com/udacity/deep-reinforcement-learning/blob/master/dqn/solution/dqn_agent.py\n",
    "        for target, runner in zip(self.target_network.parameters(), self.runner_network.parameters()):\n",
    "            target.data.copy_(self.tau * runner.data + (1 - self.tau) * target.data)\n",
    "    \n",
    "    def optimize(self, x, y):\n",
    "        loss_func = torch.nn.SmoothL1Loss() # Huber loss\n",
    "        #loss_func = torch.nn.MSELoss()\n",
    "        loss = loss_func(x, y)\n",
    "        self.runner_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.runner_optimizer.step()\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        return np.argmax(self.runner_network(torch.tensor(state, requires_grad=False, dtype=torch.float32)).cpu().detach().numpy())\n",
    "    \n",
    "    def max_q(self, state):\n",
    "        return np.max(self.runner_network(torch.tensor(state, requires_grad=False, dtype=torch.float32)).cpu().detach().numpy())\n",
    "        \n",
    "    \n",
    "    def append_replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def get_batch(self):\n",
    "        batch = random.sample(self.replay_memory, self.batch_size)\n",
    "        state, next_state = np.empty([self.batch_size, self.state_size*4]), np.empty([self.batch_size, self.state_size*4])\n",
    "        action, reward, done = [], [], []\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = batch[i][0]\n",
    "            action.append(batch[i][1])\n",
    "            reward.append(batch[i][2])\n",
    "            next_state[i] = batch[i][3]\n",
    "            done.append(batch[i][4])\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def train(self):\n",
    "        s, a, r, s_n, d = self.get_batch()\n",
    "        tensor_s = torch.tensor(s, requires_grad=False, dtype=torch.float32)\n",
    "        tensor_s_n = torch.tensor(s_n, requires_grad=False, dtype=torch.float32)\n",
    "        target_q = self.target_network(tensor_s_n).cpu()\n",
    "        runner_q = self.runner_network(tensor_s).cpu()\n",
    "        \n",
    "        update_target = np.empty([self.state_size, self.action_size])\n",
    "        update_target = target_q.detach().numpy()\n",
    "        \n",
    "        q = torch.tensor(target_q)\n",
    "        for i in range(self.batch_size):\n",
    "            if d[i] is True:\n",
    "                update_target[i][a[i]] = r[i]\n",
    "            else:\n",
    "                update_target[i][a[i]] = r[i] + self.discount_factor * torch.max(target_q[i])\n",
    "        \n",
    "\n",
    "        current = torch.tensor(runner_q, requires_grad=True, dtype=torch.float32)\n",
    "        target = torch.tensor(update_target, requires_grad=False, dtype=torch.float32)\n",
    "        \n",
    "        loss_func = torch.nn.SmoothL1Loss()\n",
    "        loss = loss_func(current, target)\n",
    "        self.runner_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.runner_optimizer.step()\n",
    "    \n",
    "    def save_model(self, p):\n",
    "        torch.save(self.runner_network.state_dict(), p)\n",
    "    \n",
    "    def restore_model(self, p):\n",
    "        self.runner_network.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. Define a network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network():\n",
    "    act = torch.nn.ReLU\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(37*4, 64),\n",
    "                           act(),\n",
    "                           torch.nn.Linear(64, 64),\n",
    "                           act(),\n",
    "                           torch.nn.Linear(64, 4))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. Build the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-5\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "REPLAY_MEMORY_SIZE = 15000\n",
    "BATCH_SIZE = 64\n",
    "EPSILON_LOWER_BOUND = 0.008\n",
    "EPSILON_DECAY_RATIO = 0.90\n",
    "TOTAL_EPISODES = 4000\n",
    "OBSERVATION_STEP = 100\n",
    "MODEL_UPDATE_EPISODE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size=37, action_size=4, build_network=network(), lr=LEARNING_RATE,\n",
    "                discount_factor=DISCOUNT_FACTOR, replay_mem_size=REPLAY_MEMORY_SIZE, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Environment initialized.\n",
      "[Step 00000301; Episode 000000] reward: -1.0   max Q: 0.21600577  ε= 0.9998172148148251\n",
      "[Step 00012301; Episode 000040] reward: 0.0   max Q: 0.94444877  ε= 0.9887949925932235\n",
      "[Step 00024301; Episode 000080] reward: 0.0   max Q: 0.66333264  ε= 0.9777727703716218\n",
      "[Step 00036301; Episode 000120] reward: -1.0   max Q: 0.6948799  ε= 0.9667505481500202  Avg. score: -0.11\n",
      "[Step 00048301; Episode 000160] reward: 2.0   max Q: 0.7181372  ε= 0.9557283259284185  Avg. score: 0.07\n",
      "[Step 00060301; Episode 000200] reward: 1.0   max Q: 0.61221194  ε= 0.9447061037068168  Avg. score: 0.06\n",
      "[Step 00072301; Episode 000240] reward: 0.0   max Q: 0.68924487  ε= 0.9336838814852152  Avg. score: -0.07\n",
      "[Step 00084301; Episode 000280] reward: -1.0   max Q: 0.66173375  ε= 0.9226616592636135  Avg. score: 0.11\n",
      "[Step 00096301; Episode 000320] reward: -2.0   max Q: 0.63772476  ε= 0.9116394370420119  Avg. score: 0.18\n",
      "[Step 00108301; Episode 000360] reward: 1.0   max Q: 0.5879095  ε= 0.9006172148204102  Avg. score: 0.16\n",
      "[Step 00120301; Episode 000400] reward: 2.0   max Q: 0.7016697  ε= 0.8895949925988086  Avg. score: 0.27\n",
      "[Step 00132301; Episode 000440] reward: -2.0   max Q: 0.66544014  ε= 0.8785727703772069  Avg. score: 0.19\n",
      "[Step 00144301; Episode 000480] reward: 0.0   max Q: 0.52544415  ε= 0.8675505481556053  Avg. score: 0.02\n",
      "[Step 00156301; Episode 000520] reward: 0.0   max Q: 0.7591032  ε= 0.8565283259340036  Avg. score: 0.18\n",
      "[Step 00168301; Episode 000560] reward: -2.0   max Q: 0.76825726  ε= 0.845506103712402  Avg. score: 0.22\n",
      "[Step 00180301; Episode 000600] reward: 0.0   max Q: 0.73542315  ε= 0.8344838814908003  Avg. score: 0.23\n",
      "[Step 00192301; Episode 000640] reward: -2.0   max Q: 0.93827033  ε= 0.8234616592691987  Avg. score: 0.2\n",
      "[Step 00204301; Episode 000680] reward: 1.0   max Q: 0.81207085  ε= 0.812439437047597  Avg. score: 0.27\n",
      "[Step 00216301; Episode 000720] reward: 0.0   max Q: 0.6354707  ε= 0.8014172148259954  Avg. score: 0.24\n",
      "[Step 00228301; Episode 000760] reward: 2.0   max Q: 0.49863046  ε= 0.7903949926043937  Avg. score: 0.07\n",
      "[Step 00240301; Episode 000800] reward: 1.0   max Q: 0.6793409  ε= 0.7793727703827921  Avg. score: -0.08\n",
      "[Step 00252301; Episode 000840] reward: -1.0   max Q: 0.8148734  ε= 0.7683505481611904  Avg. score: 0.09\n",
      "[Step 00264301; Episode 000880] reward: 3.0   max Q: 0.71930015  ε= 0.7573283259395888  Avg. score: 0.33\n",
      "[Step 00276301; Episode 000920] reward: 2.0   max Q: 0.74346733  ε= 0.7463061037179871  Avg. score: 0.25\n",
      "[Step 00288301; Episode 000960] reward: 0.0   max Q: 0.7727306  ε= 0.7352838814963855  Avg. score: 0.26\n",
      "[Step 00300301; Episode 001000] reward: -1.0   max Q: 0.9703084  ε= 0.7242616592747838  Avg. score: 0.35\n",
      "[Step 00312301; Episode 001040] reward: 0.0   max Q: 1.0977826  ε= 0.7132394370531822  Avg. score: 0.31\n",
      "[Step 00324301; Episode 001080] reward: -1.0   max Q: 1.4364319  ε= 0.7022172148315805  Avg. score: 0.45\n",
      "[Step 00336301; Episode 001120] reward: 1.0   max Q: 0.8960451  ε= 0.6911949926099789  Avg. score: 0.5\n",
      "[Step 00348301; Episode 001160] reward: 2.0   max Q: 0.92002904  ε= 0.6801727703883772  Avg. score: 0.63\n",
      "[Step 00360301; Episode 001200] reward: 2.0   max Q: 1.3269619  ε= 0.6691505481667755  Avg. score: 0.93\n",
      "[Step 00372301; Episode 001240] reward: 1.0   max Q: 1.3306422  ε= 0.6581283259451739  Avg. score: 0.93\n",
      "[Step 00384301; Episode 001280] reward: 1.0   max Q: 1.0129757  ε= 0.6471061037235722  Avg. score: 0.89\n",
      "[Step 00396301; Episode 001320] reward: 0.0   max Q: 1.0544806  ε= 0.6360838815019706  Avg. score: 0.56\n",
      "[Step 00408301; Episode 001360] reward: 1.0   max Q: 1.2337216  ε= 0.6250616592803689  Avg. score: 0.62\n",
      "[Step 00420301; Episode 001400] reward: 0.0   max Q: 1.3899201  ε= 0.6140394370587673  Avg. score: 0.8\n",
      "[Step 00432301; Episode 001440] reward: 2.0   max Q: 1.4825431  ε= 0.6030172148371656  Avg. score: 1.02\n",
      "[Step 00444301; Episode 001480] reward: 2.0   max Q: 1.1331161  ε= 0.591994992615564  Avg. score: 1.05\n",
      "[Step 00456301; Episode 001520] reward: 0.0   max Q: 1.2637968  ε= 0.5809727703939623  Avg. score: 0.92\n",
      "[Step 00468301; Episode 001560] reward: 3.0   max Q: 1.2202823  ε= 0.5699505481723607  Avg. score: 1.07\n",
      "[Step 00480301; Episode 001600] reward: 3.0   max Q: 1.7444259  ε= 0.558928325950759  Avg. score: 1.38\n",
      "[Step 00492301; Episode 001640] reward: 0.0   max Q: 1.9578627  ε= 0.5479061037291574  Avg. score: 1.66\n",
      "[Step 00504301; Episode 001680] reward: 4.0   max Q: 2.012991  ε= 0.5368838815075557  Avg. score: 2.08\n",
      "[Step 00516301; Episode 001720] reward: 5.0   max Q: 2.217937  ε= 0.5258616592859541  Avg. score: 1.98\n",
      "[Step 00528301; Episode 001760] reward: 2.0   max Q: 2.0764048  ε= 0.5148394370643524  Avg. score: 2.11\n",
      "[Step 00540301; Episode 001800] reward: 0.0   max Q: 2.5586178  ε= 0.5038172148427508  Avg. score: 1.92\n",
      "[Step 00552301; Episode 001840] reward: 6.0   max Q: 2.2426813  ε= 0.49279499262071363  Avg. score: 2.17\n",
      "[Step 00564301; Episode 001880] reward: 1.0   max Q: 2.5859432  ε= 0.48177277039844585  Avg. score: 2.41\n",
      "[Step 00576301; Episode 001920] reward: 2.0   max Q: 2.2550159  ε= 0.47075054817617806  Avg. score: 2.56\n",
      "[Step 00588301; Episode 001960] reward: 0.0   max Q: 2.4010036  ε= 0.4597283259539103  Avg. score: 2.48\n",
      "[Step 00600301; Episode 002000] reward: 1.0   max Q: 2.0676966  ε= 0.4487061037316425  Avg. score: 2.15\n",
      "[Step 00612301; Episode 002040] reward: 1.0   max Q: 1.9821028  ε= 0.4376838815093747  Avg. score: 2.12\n",
      "[Step 00624301; Episode 002080] reward: 2.0   max Q: 2.5217378  ε= 0.4266616592871069  Avg. score: 2.4\n",
      "[Step 00636301; Episode 002120] reward: 8.0   max Q: 2.9733558  ε= 0.41563943706483913  Avg. score: 2.71\n",
      "[Step 00648301; Episode 002160] reward: 4.0   max Q: 2.7000306  ε= 0.40461721484257135  Avg. score: 3.01\n",
      "[Step 00660301; Episode 002200] reward: 4.0   max Q: 3.4027114  ε= 0.39359499262030356  Avg. score: 3.1\n",
      "[Step 00672301; Episode 002240] reward: 6.0   max Q: 2.8651814  ε= 0.3825727703980358  Avg. score: 3.11\n",
      "[Step 00684301; Episode 002280] reward: 8.0   max Q: 3.0078435  ε= 0.371550548175768  Avg. score: 3.19\n",
      "[Step 00696301; Episode 002320] reward: 3.0   max Q: 2.9421434  ε= 0.3605283259535002  Avg. score: 3.1\n",
      "[Step 00708301; Episode 002360] reward: 4.0   max Q: 2.9487576  ε= 0.3495061037312324  Avg. score: 3.08\n",
      "[Step 00720301; Episode 002400] reward: 8.0   max Q: 3.5828512  ε= 0.33848388150896463  Avg. score: 3.38\n",
      "[Step 00732301; Episode 002440] reward: 2.0   max Q: 4.456248  ε= 0.32746165928669685  Avg. score: 4.07\n",
      "[Step 00744301; Episode 002480] reward: 4.0   max Q: 3.4887707  ε= 0.31643943706442906  Avg. score: 4.52\n",
      "[Step 00756301; Episode 002520] reward: 6.0   max Q: 3.9754138  ε= 0.3054172148421613  Avg. score: 4.21\n",
      "[Step 00768301; Episode 002560] reward: 4.0   max Q: 3.3401527  ε= 0.2943949926198935  Avg. score: 4.15\n",
      "[Step 00780301; Episode 002600] reward: 0.0   max Q: 3.5421448  ε= 0.2833727703976257  Avg. score: 4.1\n",
      "[Step 00792301; Episode 002640] reward: 3.0   max Q: 3.7520225  ε= 0.2723505481753579  Avg. score: 4.01\n",
      "[Step 00804301; Episode 002680] reward: 9.0   max Q: 3.1545618  ε= 0.26132832595309013  Avg. score: 3.93\n",
      "[Step 00816301; Episode 002720] reward: 4.0   max Q: 3.7141085  ε= 0.25030610373082235  Avg. score: 4.52\n",
      "[Step 00828301; Episode 002760] reward: 2.0   max Q: 3.7254179  ε= 0.23928388150855456  Avg. score: 4.44\n",
      "[Step 00840301; Episode 002800] reward: 4.0   max Q: 4.463083  ε= 0.22826165928628678  Avg. score: 4.66\n",
      "[Step 00852301; Episode 002840] reward: 3.0   max Q: 4.7797236  ε= 0.217239437064019  Avg. score: 5.0\n",
      "[Step 00864301; Episode 002880] reward: 8.0   max Q: 4.5642457  ε= 0.2062172148417512  Avg. score: 5.54\n",
      "[Step 00876301; Episode 002920] reward: 8.0   max Q: 3.7439175  ε= 0.19519499261948342  Avg. score: 5.51\n",
      "[Step 00888301; Episode 002960] reward: 3.0   max Q: 3.322866  ε= 0.18417277039721563  Avg. score: 4.86\n",
      "[Step 00900301; Episode 003000] reward: 5.0   max Q: 4.1654096  ε= 0.17315054817494785  Avg. score: 5.13\n",
      "[Step 00912301; Episode 003040] reward: 8.0   max Q: 4.022519  ε= 0.16212832595268006  Avg. score: 5.3\n",
      "[Step 00924301; Episode 003080] reward: 8.0   max Q: 4.7603197  ε= 0.15110610373041228  Avg. score: 6.03\n",
      "[Step 00936301; Episode 003120] reward: 10.0   max Q: 4.6653104  ε= 0.1400838815081445  Avg. score: 6.18\n",
      "[Step 00948301; Episode 003160] reward: 1.0   max Q: 3.9672213  ε= 0.1290616592858767  Avg. score: 6.27\n",
      "[Step 00960301; Episode 003200] reward: 4.0   max Q: 4.7669597  ε= 0.11803943706360892  Avg. score: 6.15\n",
      "[Step 00972301; Episode 003240] reward: 9.0   max Q: 4.445167  ε= 0.10701721484134114  Avg. score: 6.25\n",
      "[Step 00984301; Episode 003280] reward: 10.0   max Q: 5.039155  ε= 0.09599499261907335  Avg. score: 6.82\n",
      "[Step 00996301; Episode 003320] reward: 4.0   max Q: 4.951373  ε= 0.08497277039680556  Avg. score: 7.3\n",
      "[Step 01008301; Episode 003360] reward: 10.0   max Q: 5.7011714  ε= 0.07395054817453778  Avg. score: 7.66\n",
      "[Step 01020301; Episode 003400] reward: 7.0   max Q: 5.147009  ε= 0.06292832595226999  Avg. score: 7.82\n",
      "[Step 01032301; Episode 003440] reward: 1.0   max Q: 5.4514894  ε= 0.05190610373008224  Avg. score: 7.54\n",
      "[Step 01044301; Episode 003480] reward: 10.0   max Q: 5.425832  ε= 0.04088388150789772  Avg. score: 8.19\n",
      "[Step 01056301; Episode 003520] reward: 14.0   max Q: 4.7191367  ε= 0.029861659285707956  Avg. score: 8.68\n",
      "[Step 01068301; Episode 003560] reward: 10.0   max Q: 4.9027624  ε= 0.018839437063481804  Avg. score: 8.16\n",
      "[Step 01080301; Episode 003600] reward: 1.0   max Q: 5.9054074  ε= 0.007999081507922383  Avg. score: 8.76\n",
      "[Step 01092301; Episode 003640] reward: 4.0   max Q: 5.958706  ε= 0.007999081507922383  Avg. score: 8.92\n",
      "[Step 01104301; Episode 003680] reward: 4.0   max Q: 6.1202354  ε= 0.007999081507922383  Avg. score: 9.86\n",
      "[Step 01116301; Episode 003720] reward: 14.0   max Q: 5.653842  ε= 0.007999081507922383  Avg. score: 9.79\n",
      "[Step 01128301; Episode 003760] reward: 6.0   max Q: 6.019223  ε= 0.007999081507922383  Avg. score: 9.62\n",
      "[Step 01140301; Episode 003800] reward: 6.0   max Q: 4.635744  ε= 0.007999081507922383  Avg. score: 9.08\n",
      "[Step 01152301; Episode 003840] reward: 13.0   max Q: 5.4591556  ε= 0.007999081507922383  Avg. score: 8.83\n",
      "[Step 01164301; Episode 003880] reward: 10.0   max Q: 5.948785  ε= 0.007999081507922383  Avg. score: 9.37\n",
      "[Step 01176301; Episode 003920] reward: 9.0   max Q: 4.6624513  ε= 0.007999081507922383  Avg. score: 8.57\n",
      "[Step 01188301; Episode 003960] reward: 13.0   max Q: 4.8844013  ε= 0.007999081507922383  Avg. score: 8.8\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64\\\\Banana.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]  # Refer to the code \"1. Start the Environment\" above.\n",
    "print('[INFO] Environment initialized.')\n",
    "\n",
    "scores = []\n",
    "steps = 0\n",
    "epsilon = 1\n",
    "epsilon_decay = (1 - EPSILON_LOWER_BOUND) / ((TOTAL_EPISODES * 300) * EPSILON_DECAY_RATIO)\n",
    "avg_reward = collections.deque(maxlen=100)\n",
    "state_data = np.empty([37*4])\n",
    "for e in range(TOTAL_EPISODES):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    if e is 0:\n",
    "        state_data = np.stack([state, state, state, state]).flatten()\n",
    "    score = 0\n",
    "    while True:\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, 4)\n",
    "        else:\n",
    "            action = agent.get_action(state_data)\n",
    "        env_info = env.step(int(action))[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        next_state = np.append(next_state, state_data[0:-37])\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        \n",
    "        agent.append_replay_memory(state_data, action, reward, next_state, done)\n",
    "        \n",
    "        if steps > OBSERVATION_STEP:\n",
    "            agent.train()\n",
    "            agent.update_target_network_smooth()\n",
    "            if epsilon > EPSILON_LOWER_BOUND:\n",
    "                epsilon -= epsilon_decay\n",
    "        score += reward\n",
    "        state_data = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            scores.append(score)\n",
    "            avg_reward.append(score)\n",
    "            break\n",
    "             \n",
    "    if e % 40 == 0:\n",
    "        max_q = agent.max_q(state_data)\n",
    "        if not e >= 100:\n",
    "            print('[Step ' + str(steps + 1).zfill(8) + '; Episode ' + str(e).zfill(6) + '] reward: ' + str(score),\n",
    "                  \"  max Q: \" + str(max_q) +\n",
    "                  \"  ε= \" + str(epsilon))\n",
    "        else:\n",
    "            print('[Step ' + str(steps + 1).zfill(8) + '; Episode ' + str(e).zfill(6) + '] reward: ' + str(score),\n",
    "                  \"  max Q: \" + str(max_q) +\n",
    "                  \"  ε= \" + str(epsilon) + \n",
    "                 \"  Avg. score: \" + str(np.average(avg_reward)))\n",
    "    \n",
    "   # if e % MODEL_UPDATE_EPISODE == 0:\n",
    "        #agent.update_target_network()\n",
    "    \n",
    "    if len(avg_reward) is 100:\n",
    "        avg = np.average(avg_reward)\n",
    "        if avg > 13:\n",
    "            print(\"[INFO] Training completed. Total episode: \" + str(e))\n",
    "            break\n",
    "\n",
    "with open('save\\\\scores.pickle', 'wb') as f:\n",
    "    pickle.dump(scores, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "agent.save_model('save\\\\model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-2. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size=37, action_size=4, build_network=network(), lr=LEARNING_RATE,\n",
    "                discount_factor=DISCOUNT_FACTOR, replay_mem_size=REPLAY_MEMORY_SIZE, batch_size=BATCH_SIZE)\n",
    "agent.restore_model('save\\\\model')\n",
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64\\\\Banana.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]  # Refer to the code \"1. Start the Environment\" above.\n",
    "print('[INFO] Environment initialized.')\n",
    "\n",
    "steps = 0\n",
    "avg_reward = collections.deque(maxlen=100)\n",
    "for e in range(100):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    score = 0\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        env_info = env.step(int(action))[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        \n",
    "        score += reward\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            avg_reward.append(score)\n",
    "            break\n",
    "             \n",
    "    if e % 20 == 0:\n",
    "            print('Episode ' + str(e).zfill(3) + ' reward: ' + str(score))\n",
    "\n",
    "print(\"Avg(100 episode) score: \" + str(np.average(avg_reward)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5. Visualising the training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(range(len(scores)), scores)\n",
    "ax.set(xlabel='Episode', ylabel=\"total score\")\n",
    "plt.title(\"Total Reward\")\n",
    "plt.savefig(\"save\\\\reward.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'd like to solve this project with other reinforcement learning methods, such as Dueling DQN, Double DQN, DRQN, and so on.\n",
    "- I'd like to use DQN-based network in video game invironment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
