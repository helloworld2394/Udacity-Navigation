{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64\\\\Banana.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dqn\"></a>\n",
    "***************\n",
    "#### 4-1. Definite a DQN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import random\n",
    "import collections\n",
    "import pickle\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, build_network, lr=0.001, discount_factor=.99,\n",
    "                 replay_mem_size=2500, batch_size=32):\n",
    "        \n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        torch.cuda.set_device(0)\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.replay_memory = collections.deque(maxlen=replay_mem_size)\n",
    "        self.runner_network = build_network.cuda()\n",
    "        self.target_network = build_network.cuda()\n",
    "\n",
    "        self.runner_optimizer = torch.optim.RMSprop(self.runner_network.parameters(), lr=self.lr)\n",
    "        \n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.runner_network.state_dict())\n",
    "    \n",
    "    def optimize(self, x, y):\n",
    "        loss_func = torch.nn.SmoothL1Loss() # Huber loss\n",
    "        #loss_func = torch.nn.MSELoss()\n",
    "        loss = loss_func(x, y)\n",
    "        self.runner_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.runner_optimizer.step()\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        self.runner_network.eval()\n",
    "        return np.argmax(self.runner_network(torch.tensor([state], requires_grad=False, dtype=torch.float32)).cpu().detach().numpy())\n",
    "    \n",
    "    def max_q(self, state):\n",
    "        self.runner_network.eval()\n",
    "        return np.max(self.runner_network(torch.tensor([state], requires_grad=False, dtype=torch.float32)).cpu().detach().numpy())\n",
    "        \n",
    "    \n",
    "    def append_replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def get_batch(self):\n",
    "        batch = random.sample(self.replay_memory, self.batch_size)\n",
    "        state, next_state = np.empty([self.batch_size, self.state_size*4]), np.empty([self.batch_size, self.state_size*4])\n",
    "        action, reward, done = [], [], []\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = batch[i][0]\n",
    "            action.append(batch[i][1])\n",
    "            reward.append(batch[i][2])\n",
    "            next_state[i] = batch[i][3]\n",
    "            done.append(batch[i][4])\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def train(self):\n",
    "        self.runner_network.train()\n",
    "        s, a, r, s_n, d = self.get_batch()\n",
    "        tensor_s = torch.tensor(s, requires_grad=False, dtype=torch.float32)\n",
    "        tensor_s_n = torch.tensor(s_n, requires_grad=False, dtype=torch.float32)\n",
    "        target_q = self.target_network(tensor_s_n).cpu()\n",
    "        runner_q = self.runner_network(tensor_s).cpu()\n",
    "        \n",
    "        update_target = np.empty([self.state_size, self.action_size])\n",
    "        update_target = target_q.detach().numpy()\n",
    "        \n",
    "        q = torch.tensor(target_q)\n",
    "        for i in range(self.batch_size):\n",
    "            if d[i] is True:\n",
    "                update_target[i][a[i]] = r[i]\n",
    "            else:\n",
    "                update_target[i][a[i]] = r[i] + self.discount_factor * torch.max(target_q[i])\n",
    "        \n",
    "\n",
    "        current = torch.tensor(runner_q, requires_grad=True, dtype=torch.float32)\n",
    "        target = torch.tensor(update_target, requires_grad=False, dtype=torch.float32)\n",
    "        \n",
    "        loss_func = torch.nn.SmoothL1Loss()\n",
    "        loss = loss_func(current, target)\n",
    "        self.runner_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.runner_optimizer.step()\n",
    "    \n",
    "    def save_model(self, p):\n",
    "        torch.save(self.runner_network.state_dict(), p)\n",
    "    \n",
    "    def restore_model(self, p):\n",
    "        self.runner_network.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. Define a network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network():\n",
    "    act = torch.nn.ReLU\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(37*4, 64),\n",
    "                           act(),\n",
    "                           torch.nn.BatchNorm1d(64),\n",
    "                           torch.nn.Linear(64, 64),\n",
    "                           act(),\n",
    "                           torch.nn.BatchNorm1d(64),\n",
    "                           torch.nn.Linear(64, 4))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. Build the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 7e-5\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "REPLAY_MEMORY_SIZE = 15000\n",
    "BATCH_SIZE = 64\n",
    "EPSILON_DECAY = 0.9965\n",
    "TOTAL_EPISODES = 2000\n",
    "OBSERVATION_STEP = 70\n",
    "MODEL_UPDATE_EPISODE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size=37, action_size=4, build_network=network(), lr=LEARNING_RATE,\n",
    "                discount_factor=DISCOUNT_FACTOR, replay_mem_size=REPLAY_MEMORY_SIZE, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Environment initialized.\n",
      "[Step 00000300; Episode 000000] reward: -1.0   max Q: 0.09338943  ε= 1\n",
      "[Step 00012300; Episode 000040] reward: 1.0   max Q: 1.1282498  ε= 0.8691447705514587\n",
      "[Step 00024300; Episode 000080] reward: 0.0   max Q: 0.75863504  ε= 0.7554126321769475\n",
      "[Step 00036300; Episode 000120] reward: 0.0   max Q: 1.0080085  ε= 0.6565629388651066  Avg. score: 0.15\n",
      "[Step 00048300; Episode 000160] reward: 0.0   max Q: 1.5462185  ε= 0.5706482448525043  Avg. score: 0.58\n",
      "[Step 00060300; Episode 000200] reward: 2.0   max Q: 1.553146  ε= 0.4959759378379223  Avg. score: 0.97\n",
      "[Step 00072300; Episode 000240] reward: -1.0   max Q: 2.1618571  ε= 0.4310748926911856  Avg. score: 1.33\n",
      "[Step 00084300; Episode 000280] reward: 0.0   max Q: 2.322814  ε= 0.3746664886985753  Avg. score: 1.85\n",
      "[Step 00096300; Episode 000320] reward: 7.0   max Q: 2.9312093  ε= 0.32563941935324386  Avg. score: 2.73\n",
      "[Step 00108300; Episode 000360] reward: 3.0   max Q: 3.356971  ε= 0.2830277984162855  Avg. score: 3.75\n",
      "[Step 00120300; Episode 000400] reward: 2.0   max Q: 3.44596  ε= 0.245992130914207  Avg. score: 4.05\n",
      "[Step 00132300; Episode 000440] reward: 0.0   max Q: 3.7332597  ε= 0.2138027741808928  Avg. score: 4.46\n",
      "[Step 00144300; Episode 000480] reward: 11.0   max Q: 3.829774  ε= 0.18582556310871748  Avg. score: 4.73\n",
      "[Step 00156300; Episode 000520] reward: 6.0   max Q: 4.4726405  ε= 0.1615093164107218  Avg. score: 5.57\n",
      "[Step 00168300; Episode 000560] reward: 9.0   max Q: 4.518753  ε= 0.14037497775371982  Avg. score: 5.61\n",
      "[Step 00180300; Episode 000600] reward: 5.0   max Q: 4.3019443  ε= 0.12200617783092295  Avg. score: 6.0\n",
      "[Step 00192300; Episode 000640] reward: 6.0   max Q: 4.987659  ε= 0.106041031436718  Avg. score: 5.83\n",
      "[Step 00204300; Episode 000680] reward: 9.0   max Q: 4.736332  ε= 0.09216500793710629  Avg. score: 6.44\n",
      "[Step 00216300; Episode 000720] reward: 4.0   max Q: 4.6989255  ε= 0.08010473467636962  Avg. score: 6.19\n",
      "[Step 00228300; Episode 000760] reward: 2.0   max Q: 4.8742647  ε= 0.06962261124037876  Avg. score: 6.64\n",
      "[Step 00240300; Episode 000800] reward: 7.0   max Q: 5.4653096  ε= 0.060512128471712424  Avg. score: 6.94\n",
      "[Step 00252300; Episode 000840] reward: 17.0   max Q: 4.135543  ε= 0.0525938000161269  Avg. score: 7.07\n",
      "[Step 00264300; Episode 000880] reward: 4.0   max Q: 5.1014524  ε= 0.04571162624744593  Avg. score: 7.07\n",
      "[Step 00276300; Episode 000920] reward: 10.0   max Q: 5.3413997  ε= 0.039730020906370435  Avg. score: 8.3\n",
      "[Step 00288300; Episode 000960] reward: 9.0   max Q: 5.114933  ε= 0.034531139904671986  Avg. score: 8.08\n",
      "[Step 00300300; Episode 001000] reward: 5.0   max Q: 5.492803  ε= 0.030012559669326463  Avg. score: 8.34\n",
      "[Step 00312300; Episode 001040] reward: 10.0   max Q: 5.7484617  ε= 0.026085259287458716  Avg. score: 8.85\n",
      "[Step 00324300; Episode 001080] reward: 11.0   max Q: 5.4720054  ε= 0.02267186669817361  Avg. score: 8.18\n",
      "[Step 00336300; Episode 001120] reward: 4.0   max Q: 5.7673507  ε= 0.019705134379357357  Avg. score: 8.34\n",
      "[Step 00348300; Episode 001160] reward: 8.0   max Q: 6.1103168  ε= 0.017126614498832217  Avg. score: 7.97\n",
      "[Step 00360300; Episode 001200] reward: 5.0   max Q: 5.3086734  ε= 0.014885507428910809  Avg. score: 8.0\n",
      "[Step 00372300; Episode 001240] reward: 6.0   max Q: 4.9718947  ε= 0.012937660938842717  Avg. score: 7.69\n",
      "[Step 00384300; Episode 001280] reward: 8.0   max Q: 5.3507977  ε= 0.011244700348163017  Avg. score: 8.57\n",
      "[Step 00396300; Episode 001320] reward: 7.0   max Q: 5.3708496  ε= 0.009773272504024051  Avg. score: 8.34\n",
      "[Step 00408300; Episode 001360] reward: 10.0   max Q: 5.3599286  ε= 0.008494388688046865  Avg. score: 8.53\n",
      "[Step 00420300; Episode 001400] reward: 10.0   max Q: 5.205906  ε= 0.007382853507247396  Avg. score: 8.45\n",
      "[Step 00432300; Episode 001440] reward: 5.0   max Q: 5.157066  ε= 0.006416768517571567  Avg. score: 8.48\n",
      "[Step 00444300; Episode 001480] reward: 10.0   max Q: 5.080806  ε= 0.005577100800886566  Avg. score: 8.28\n",
      "[Step 00456300; Episode 001520] reward: 2.0   max Q: 5.0415187  ε= 0.004847307995928911  Avg. score: 7.99\n",
      "[Step 00468300; Episode 001560] reward: 13.0   max Q: 5.081236  ε= 0.004213012395913888  Avg. score: 8.79\n",
      "[Step 00480300; Episode 001600] reward: 8.0   max Q: 5.001277  ε= 0.0036617176921770263  Avg. score: 8.73\n",
      "[Step 00492300; Episode 001640] reward: 6.0   max Q: 5.0859704  ε= 0.003182562783391418  Avg. score: 8.83\n",
      "[Step 00504300; Episode 001680] reward: 15.0   max Q: 5.1459284  ε= 0.0027661078001363464  Avg. score: 8.74\n",
      "[Step 00516300; Episode 001720] reward: 8.0   max Q: 5.1372232  ε= 0.002404148129270105  Avg. score: 8.72\n",
      "[Step 00528300; Episode 001760] reward: 9.0   max Q: 6.3253446  ε= 0.0020895527741861845  Avg. score: 9.45\n",
      "[Step 00540300; Episode 001800] reward: 13.0   max Q: 4.940658  ε= 0.0018161238664752158  Avg. score: 9.38\n",
      "[Step 00552300; Episode 001840] reward: 7.0   max Q: 5.1319833  ε= 0.00157847456122063  Avg. score: 8.71\n",
      "[Step 00564300; Episode 001880] reward: 7.0   max Q: 5.056935  ε= 0.0013719229103334198  Avg. score: 8.87\n",
      "[Step 00576300; Episode 001920] reward: 3.0   max Q: 4.9261217  ε= 0.00119239962311603  Avg. score: 7.92\n",
      "[Step 00588300; Episode 001960] reward: 6.0   max Q: 4.784155  ε= 0.0010363678968388277  Avg. score: 8.01\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64\\\\Banana.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]  # Refer to the code \"1. Start the Environment\" above.\n",
    "print('[INFO] Environment initialized.')\n",
    "\n",
    "scores = []\n",
    "steps = 0\n",
    "epsilon = 1\n",
    "avg_reward = collections.deque(maxlen=100)\n",
    "state_data = np.empty([37*4])\n",
    "for e in range(TOTAL_EPISODES):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    if e is 0:\n",
    "        state_data = np.stack([state, state, state, state]).flatten()\n",
    "    score = 0\n",
    "    while True:\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, 4)\n",
    "        else:\n",
    "            action = agent.get_action(state_data)\n",
    "        env_info = env.step(int(action))[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        next_state = np.append(next_state, state_data[0:-37])\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        \n",
    "        agent.append_replay_memory(state_data, action, reward, next_state, done)\n",
    "        \n",
    "        if steps > OBSERVATION_STEP:\n",
    "            agent.train()\n",
    "\n",
    "        score += reward\n",
    "        state_data = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            scores.append(score)\n",
    "            avg_reward.append(score)\n",
    "            break\n",
    "             \n",
    "    if e % 40 == 0:\n",
    "        max_q = agent.max_q(state_data)\n",
    "        if not e >= 100:\n",
    "            print('[Step ' + str(steps).zfill(8) + '; Episode ' + str(e).zfill(6) + '] reward: ' + str(score),\n",
    "                  \"  max Q: \" + str(max_q) +\n",
    "                  \"  ε= \" + str(epsilon))\n",
    "        else:\n",
    "            print('[Step ' + str(steps).zfill(8) + '; Episode ' + str(e).zfill(6) + '] reward: ' + str(score),\n",
    "                  \"  max Q: \" + str(max_q) +\n",
    "                  \"  ε= \" + str(epsilon) + \n",
    "                 \"  Avg. score: \" + str(np.average(avg_reward)))\n",
    "    \n",
    "    if e % MODEL_UPDATE_EPISODE == 0:\n",
    "        agent.update_target_network()\n",
    "    \n",
    "    if len(avg_reward) is 100:\n",
    "        avg = np.average(avg_reward)\n",
    "        if avg > 13:\n",
    "            print(\"[INFO] Training completed. Total episode: \" + str(e))\n",
    "            break\n",
    "    \n",
    "    epsilon *= EPSILON_DECAY\n",
    "\n",
    "with open('save\\\\scores.pickle', 'wb') as f:\n",
    "    pickle.dump(scores, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "agent.save_model('save\\\\model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. Visualising the training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(range(len(scores)), scores)\n",
    "ax.set(xlabel='Episode', ylabel=\"total score\")\n",
    "plt.title(\"Total Reward\")\n",
    "plt.savefig(\"save\\\\reward.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'd like to solve this project with other reinforcement learning methods, such as Dueling DQN, Double DQN, DRQN, and so on.\n",
    "- I'd like to use DQN-based network in video game invironment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
