{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64\\\\Banana.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dqn\"></a>\n",
    "***************\n",
    "#### 4-1. Definite a DQN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import random\n",
    "import collections\n",
    "import pickle\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, build_network, lr=0.001, discount_factor=.99,\n",
    "                 replay_mem_size=2500, batch_size=32, smooth_update_tau=0.001):\n",
    "        \n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        torch.cuda.set_device(0)\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = smooth_update_tau\n",
    "        self.discount_factor = discount_factor\n",
    "        self.replay_memory = collections.deque(maxlen=replay_mem_size)\n",
    "        self.runner_network = build_network.cuda()\n",
    "        self.target_network = build_network.cuda()\n",
    "\n",
    "        self.runner_optimizer = torch.optim.Adam(self.runner_network.parameters(), lr=self.lr)\n",
    "        \n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.runner_network.state_dict())\n",
    "    \n",
    "    def update_target_network_smooth(self):\n",
    "        # refer to https://github.com/udacity/deep-reinforcement-learning/blob/master/dqn/solution/dqn_agent.py\n",
    "        for target, runner in zip(self.target_network.parameters(), self.runner_network.parameters()):\n",
    "            target.data.copy_(self.tau * runner.data + (1 - self.tau) * target.data)\n",
    "    \n",
    "    def optimize(self, x, y):\n",
    "        loss_func = torch.nn.SmoothL1Loss() # Huber loss\n",
    "        #loss_func = torch.nn.MSELoss()\n",
    "        loss = loss_func(x, y)\n",
    "        self.runner_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.runner_optimizer.step()\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        return np.argmax(self.runner_network(torch.tensor(state, requires_grad=False, dtype=torch.float32)).cpu().detach().numpy())\n",
    "    \n",
    "    def max_q(self, state):\n",
    "        return np.max(self.runner_network(torch.tensor(state, requires_grad=False, dtype=torch.float32)).cpu().detach().numpy())\n",
    "        \n",
    "    \n",
    "    def append_replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def get_batch(self):\n",
    "        batch = random.sample(self.replay_memory, self.batch_size)\n",
    "        state, next_state = np.empty([self.batch_size, self.state_size*4]), np.empty([self.batch_size, self.state_size*4])\n",
    "        action, reward, done = [], [], []\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = batch[i][0]\n",
    "            action.append(batch[i][1])\n",
    "            reward.append(batch[i][2])\n",
    "            next_state[i] = batch[i][3]\n",
    "            done.append(batch[i][4])\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def train(self):\n",
    "        s, a, r, s_n, d = self.get_batch()\n",
    "        tensor_s = torch.tensor(s, requires_grad=False, dtype=torch.float32)\n",
    "        tensor_s_n = torch.tensor(s_n, requires_grad=False, dtype=torch.float32)\n",
    "        target_q = self.target_network(tensor_s_n).cpu()\n",
    "        runner_q = self.runner_network(tensor_s).cpu()\n",
    "        \n",
    "        update_target = np.empty([self.state_size, self.action_size])\n",
    "        update_target = target_q.detach().numpy()\n",
    "        \n",
    "        q = torch.tensor(target_q)\n",
    "        for i in range(self.batch_size):\n",
    "            if d[i] is True:\n",
    "                update_target[i][a[i]] = r[i]\n",
    "            else:\n",
    "                update_target[i][a[i]] = r[i] + self.discount_factor * torch.max(target_q[i])\n",
    "        \n",
    "\n",
    "        current = torch.tensor(runner_q, requires_grad=True, dtype=torch.float32)\n",
    "        target = torch.tensor(update_target, requires_grad=False, dtype=torch.float32)\n",
    "        \n",
    "        loss_func = torch.nn.SmoothL1Loss()\n",
    "        loss = loss_func(current, target)\n",
    "        self.runner_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.runner_optimizer.step()\n",
    "    \n",
    "    def save_model(self, p):\n",
    "        torch.save(self.runner_network.state_dict(), p)\n",
    "    \n",
    "    def restore_model(self, p):\n",
    "        self.runner_network.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. Define a network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network():\n",
    "    act = torch.nn.ReLU\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(37*4, 64),\n",
    "                           act(),\n",
    "                           torch.nn.Linear(64, 64),\n",
    "                           act(),\n",
    "                           torch.nn.Linear(64, 4))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. Build the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-5\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "REPLAY_MEMORY_SIZE = 15000\n",
    "BATCH_SIZE = 64\n",
    "EPSILON_DECAY = 0.995\n",
    "TOTAL_EPISODES = 2000\n",
    "OBSERVATION_STEP = 70\n",
    "MODEL_UPDATE_EPISODE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size=37, action_size=4, build_network=network(), lr=LEARNING_RATE,\n",
    "                discount_factor=DISCOUNT_FACTOR, replay_mem_size=REPLAY_MEMORY_SIZE, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Environment initialized.\n",
      "[Step 00000300; Episode 000000] reward: 0.0   max Q: 0.19882722  ε= 1\n",
      "[Step 00012300; Episode 000040] reward: 0.0   max Q: 0.87492627  ε= 0.8183201210226743\n",
      "[Step 00024300; Episode 000080] reward: 1.0   max Q: 0.9560765  ε= 0.6696478204705644\n",
      "[Step 00036300; Episode 000120] reward: 0.0   max Q: 1.7106553  ε= 0.547986285490042  Avg. score: 0.66\n",
      "[Step 00048300; Episode 000160] reward: 2.0   max Q: 2.240245  ε= 0.4484282034609769  Avg. score: 1.37\n",
      "[Step 00060300; Episode 000200] reward: 2.0   max Q: 2.5649803  ε= 0.3669578217261671  Avg. score: 1.88\n",
      "[Step 00072300; Episode 000240] reward: 2.0   max Q: 2.862121  ε= 0.30028896908517405  Avg. score: 2.35\n",
      "[Step 00084300; Episode 000280] reward: 2.0   max Q: 3.490607  ε= 0.2457325055235537  Avg. score: 3.06\n",
      "[Step 00096300; Episode 000320] reward: 0.0   max Q: 3.2000227  ε= 0.2010878536592394  Avg. score: 3.78\n",
      "[Step 00108300; Episode 000360] reward: 6.0   max Q: 3.6739237  ε= 0.16455423674261854  Avg. score: 4.31\n",
      "[Step 00120300; Episode 000400] reward: 5.0   max Q: 4.0315576  ε= 0.1346580429260134  Avg. score: 4.99\n",
      "[Step 00132300; Episode 000440] reward: 2.0   max Q: 4.9365144  ε= 0.11019338598389174  Avg. score: 5.72\n",
      "[Step 00144300; Episode 000480] reward: 8.0   max Q: 4.708309  ε= 0.09017346495423652  Avg. score: 6.74\n",
      "[Step 00156300; Episode 000520] reward: 10.0   max Q: 4.719145  ε= 0.07379076075438468  Avg. score: 6.96\n",
      "[Step 00168300; Episode 000560] reward: 7.0   max Q: 4.8742867  ε= 0.06038446427088321  Avg. score: 7.4\n",
      "[Step 00180300; Episode 000600] reward: 5.0   max Q: 5.2510533  ε= 0.0494138221100385  Avg. score: 8.3\n",
      "[Step 00192300; Episode 000640] reward: 20.0   max Q: 5.5917034  ε= 0.04043632488927963  Avg. score: 8.22\n",
      "[Step 00204300; Episode 000680] reward: 2.0   max Q: 5.637381  ε= 0.03308985827710748  Avg. score: 8.27\n",
      "[Step 00216300; Episode 000720] reward: 7.0   max Q: 4.892703  ε= 0.02707809682994571  Avg. score: 8.21\n",
      "[Step 00228300; Episode 000760] reward: 7.0   max Q: 5.384896  ε= 0.022158551474944856  Avg. score: 9.16\n",
      "[Step 00240300; Episode 000800] reward: 11.0   max Q: 4.935291  ε= 0.018132788524664028  Avg. score: 9.58\n",
      "[Step 00252300; Episode 000840] reward: 10.0   max Q: 5.730353  ε= 0.014838425699981627  Avg. score: 8.7\n",
      "[Step 00264300; Episode 000880] reward: 13.0   max Q: 4.8416777  ε= 0.012142582314594924  Avg. score: 9.2\n",
      "[Step 00276300; Episode 000920] reward: 10.0   max Q: 5.445486  ε= 0.009936519429207103  Avg. score: 8.6\n",
      "[Step 00288300; Episode 000960] reward: 13.0   max Q: 4.7936635  ε= 0.008131253781852912  Avg. score: 8.98\n",
      "[Step 00300300; Episode 001000] reward: 10.0   max Q: 5.6138935  ε= 0.006653968578831948  Avg. score: 8.85\n",
      "[Step 00312300; Episode 001040] reward: 13.0   max Q: 5.5155544  ε= 0.005445076372710831  Avg. score: 8.91\n",
      "[Step 00324300; Episode 001080] reward: 14.0   max Q: 4.795381  ε= 0.00445581555629443  Avg. score: 8.88\n",
      "[Step 00336300; Episode 001120] reward: 10.0   max Q: 5.223507  ε= 0.003646283525281571  Avg. score: 9.64\n",
      "[Step 00348300; Episode 001160] reward: 12.0   max Q: 5.4096775  ε= 0.002983827175691398  Avg. score: 9.32\n",
      "[Step 00360300; Episode 001200] reward: 12.0   max Q: 5.6990867  ε= 0.002441725815522529  Avg. score: 9.53\n",
      "[Step 00372300; Episode 001240] reward: 0.0   max Q: 6.462639  ε= 0.0019981133648625847  Avg. score: 9.82\n",
      "[Step 00384300; Episode 001280] reward: 1.0   max Q: 5.3027663  ε= 0.0016350963705513723  Avg. score: 9.91\n",
      "[Step 00396300; Episode 001320] reward: 9.0   max Q: 5.2598257  ε= 0.0013380322598333348  Avg. score: 9.76\n",
      "[Step 00408300; Episode 001360] reward: 7.0   max Q: 5.53889  ε= 0.0010949387207990578  Avg. score: 10.31\n",
      "[Step 00420300; Episode 001400] reward: 13.0   max Q: 5.050123  ε= 0.0008960103865166974  Avg. score: 9.7\n",
      "[Step 00432300; Episode 001440] reward: 10.0   max Q: 4.954477  ε= 0.0007332233279319168  Avg. score: 9.48\n",
      "[Step 00444300; Episode 001480] reward: 15.0   max Q: 5.2251525  ε= 0.000600011402449894  Avg. score: 9.08\n",
      "[Step 00456300; Episode 001520] reward: 15.0   max Q: 5.763592  ε= 0.0004910014034677818  Avg. score: 9.83\n",
      "[Step 00468300; Episode 001560] reward: 4.0   max Q: 5.913204  ε= 0.000401796327908058  Avg. score: 9.85\n",
      "[Step 00480300; Episode 001600] reward: 17.0   max Q: 5.7642064  ε= 0.0003287980196801882  Avg. score: 10.19\n",
      "[Step 00492300; Episode 001640] reward: 12.0   max Q: 4.8512864  ε= 0.00026906203525670736  Avg. score: 9.8\n",
      "[Step 00504300; Episode 001680] reward: 4.0   max Q: 5.439574  ε= 0.00022017887725387584  Avg. score: 9.83\n",
      "[Step 00516300; Episode 001720] reward: 12.0   max Q: 5.642536  ε= 0.00018017680548102822  Avg. score: 9.61\n",
      "[Step 00528300; Episode 001760] reward: 18.0   max Q: 5.664798  ε= 0.00014744230526671386  Avg. score: 9.67\n",
      "[Step 00540300; Episode 001800] reward: 9.0   max Q: 5.618913  ε= 0.0001206550050897193  Avg. score: 9.51\n",
      "[Step 00552300; Episode 001840] reward: 8.0   max Q: 4.790328  ε= 9.873441836701049e-05  Avg. score: 9.06\n",
      "[Step 00564300; Episode 001880] reward: 10.0   max Q: 5.638129  ε= 8.079636118719534e-05  Avg. score: 8.73\n",
      "[Step 00576300; Episode 001920] reward: 6.0   max Q: 5.4675894  ε= 6.61172880648974e-05  Avg. score: 9.71\n",
      "[Step 00588300; Episode 001960] reward: 15.0   max Q: 4.959024  ε= 5.410510717095784e-05  Avg. score: 9.55\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64\\\\Banana.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]  # Refer to the code \"1. Start the Environment\" above.\n",
    "print('[INFO] Environment initialized.')\n",
    "\n",
    "scores = []\n",
    "steps = 0\n",
    "epsilon = 1\n",
    "avg_reward = collections.deque(maxlen=100)\n",
    "state_data = np.empty([37*4])\n",
    "for e in range(TOTAL_EPISODES):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    if e is 0:\n",
    "        state_data = np.stack([state, state, state, state]).flatten()\n",
    "    score = 0\n",
    "    while True:\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, 4)\n",
    "        else:\n",
    "            action = agent.get_action(state_data)\n",
    "        env_info = env.step(int(action))[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        next_state = np.append(next_state, state_data[0:-37])\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        \n",
    "        agent.append_replay_memory(state_data, action, reward, next_state, done)\n",
    "        \n",
    "        if steps > OBSERVATION_STEP:\n",
    "            agent.train()\n",
    "            agent.update_target_network_smooth()\n",
    "\n",
    "        score += reward\n",
    "        state_data = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            scores.append(score)\n",
    "            avg_reward.append(score)\n",
    "            break\n",
    "             \n",
    "    if e % 40 == 0:\n",
    "        max_q = agent.max_q(state_data)\n",
    "        if not e >= 100:\n",
    "            print('[Step ' + str(steps).zfill(8) + '; Episode ' + str(e).zfill(6) + '] reward: ' + str(score),\n",
    "                  \"  max Q: \" + str(max_q) +\n",
    "                  \"  ε= \" + str(epsilon))\n",
    "        else:\n",
    "            print('[Step ' + str(steps).zfill(8) + '; Episode ' + str(e).zfill(6) + '] reward: ' + str(score),\n",
    "                  \"  max Q: \" + str(max_q) +\n",
    "                  \"  ε= \" + str(epsilon) + \n",
    "                 \"  Avg. score: \" + str(np.average(avg_reward)))\n",
    "    \n",
    "   # if e % MODEL_UPDATE_EPISODE == 0:\n",
    "        #agent.update_target_network()\n",
    "    \n",
    "    if len(avg_reward) is 100:\n",
    "        avg = np.average(avg_reward)\n",
    "        if avg > 13:\n",
    "            print(\"[INFO] Training completed. Total episode: \" + str(e))\n",
    "            break\n",
    "    \n",
    "    epsilon *= EPSILON_DECAY\n",
    "\n",
    "with open('save\\\\scores.pickle', 'wb') as f:\n",
    "    pickle.dump(scores, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "agent.save_model('save\\\\model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-2. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Environment initialized.\n",
      "Episode 000 reward: 7.0\n",
      "Episode 020 reward: 9.0\n",
      "Episode 040 reward: 10.0\n",
      "Episode 060 reward: 3.0\n",
      "Episode 080 reward: 15.0\n",
      "Avg(100 episode) score: 8.07\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(state_size=37, action_size=4, build_network=network(), lr=LEARNING_RATE,\n",
    "                discount_factor=DISCOUNT_FACTOR, replay_mem_size=REPLAY_MEMORY_SIZE, batch_size=BATCH_SIZE)\n",
    "agent.restore_model('save\\\\model')\n",
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64\\\\Banana.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]  # Refer to the code \"1. Start the Environment\" above.\n",
    "print('[INFO] Environment initialized.')\n",
    "\n",
    "state_data = np.empty(37*4)\n",
    "avg_reward = collections.deque(maxlen=100)\n",
    "for e in range(100):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    if e is 0:\n",
    "        state_data = np.stack([state, state, state, state]).flatten()\n",
    "    score = 0\n",
    "    while True:\n",
    "        action = agent.get_action(state_data)\n",
    "        env_info = env.step(int(action))[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        state_data = np.append(next_state, state_data[0:-37])\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        if done:\n",
    "            avg_reward.append(score)\n",
    "            break\n",
    "             \n",
    "    if e % 20 == 0:\n",
    "            print('Episode ' + str(e).zfill(3) + ' reward: ' + str(score))\n",
    "\n",
    "print(\"Avg(100 episode) score: \" + str(np.average(avg_reward)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5. Visualising the training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(range(len(scores)), scores)\n",
    "ax.set(xlabel='Episode', ylabel=\"total score\")\n",
    "plt.title(\"Total Reward\")\n",
    "plt.savefig(\"save\\\\reward.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'd like to solve this project with other reinforcement learning methods, such as Dueling DQN, Double DQN, DRQN, and so on.\n",
    "- I'd like to use DQN-based network in video game invironment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
