{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Implementation Report\n",
    "\n",
    "---\n",
    "\n",
    "## 4-1. Import Some Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import random\n",
    "import collections\n",
    "import pickle\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, build_network, lr=0.001, discount_factor=.99,\n",
    "                 replay_mem_size=2500, batch_size=32, smooth_update_tau=0.001):\n",
    "        \n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')  # set default tensor type\n",
    "        torch.cuda.set_device(0) # use gpu\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = smooth_update_tau\n",
    "        self.discount_factor = discount_factor\n",
    "        self.replay_memory = collections.deque(maxlen=replay_mem_size)\n",
    "        self.runner_network = build_network.cuda()\n",
    "        self.target_network = build_network.cuda()\n",
    "\n",
    "        self.runner_optimizer = torch.optim.Adam(self.runner_network.parameters(), lr=self.lr)\n",
    "        \n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.runner_network.state_dict()) # copy network to target\n",
    "    \n",
    "    def update_target_network_smooth(self):\n",
    "        # refer to https://github.com/udacity/deep-reinforcement-learning/blob/master/dqn/solution/dqn_agent.py\n",
    "        for target, runner in zip(self.target_network.parameters(), self.runner_network.parameters()):\n",
    "            target.data.copy_(self.tau * runner.data + (1 - self.tau) * target.data)\n",
    "    \n",
    "    def optimize(self, x, y):\n",
    "        loss_func = torch.nn.SmoothL1Loss() # Huber loss\n",
    "        #loss_func = torch.nn.MSELoss()\n",
    "        loss = loss_func(x, y)\n",
    "        self.runner_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.runner_optimizer.step()\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        return np.argmax(self.runner_network(torch.tensor(state, requires_grad=False, dtype=torch.float32)).cpu().detach().numpy())\n",
    "    \n",
    "    def max_q(self, state):\n",
    "        return np.max(self.runner_network(torch.tensor(state, requires_grad=False, dtype=torch.float32)).cpu().detach().numpy())\n",
    "        \n",
    "    \n",
    "    def append_replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def get_batch(self):\n",
    "        batch = random.sample(self.replay_memory, self.batch_size)  # sample the replay memory\n",
    "        state, next_state = np.empty([self.batch_size, self.state_size*4]), np.empty([self.batch_size, self.state_size*4])\n",
    "        action, reward, done = [], [], []\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = batch[i][0]  # fill state data\n",
    "            action.append(batch[i][1])\n",
    "            reward.append(batch[i][2])\n",
    "            next_state[i] = batch[i][3]  # fill next_state data\n",
    "            done.append(batch[i][4])\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def train(self):\n",
    "        s, a, r, s_n, d = self.get_batch()\n",
    "        tensor_s = torch.tensor(s, requires_grad=False, dtype=torch.float32)\n",
    "        tensor_s_n = torch.tensor(s_n, requires_grad=False, dtype=torch.float32)\n",
    "        target_q = self.target_network(tensor_s_n).cpu()\n",
    "        runner_q = self.runner_network(tensor_s).cpu()\n",
    "        \n",
    "        update_target = np.empty([self.state_size, self.action_size])\n",
    "        update_target = target_q.detach().numpy()\n",
    "        \n",
    "        q = torch.tensor(target_q)\n",
    "        for i in range(self.batch_size):\n",
    "            if d[i] is True:\n",
    "                update_target[i][a[i]] = r[i]\n",
    "            else:\n",
    "                update_target[i][a[i]] = r[i] + self.discount_factor * torch.max(target_q[i])\n",
    "        \n",
    "\n",
    "        current = torch.tensor(runner_q, requires_grad=True, dtype=torch.float32)\n",
    "        target = torch.tensor(update_target, requires_grad=False, dtype=torch.float32)\n",
    "        \n",
    "        loss_func = torch.nn.SmoothL1Loss()\n",
    "        loss = loss_func(current, target)\n",
    "        self.runner_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.runner_optimizer.step()\n",
    "    \n",
    "    def save_model(self, p):\n",
    "        torch.save(self.runner_network.state_dict(), p)\n",
    "    \n",
    "    def restore_model(self, p):\n",
    "        self.runner_network.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. Define a network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "My network is\n",
    "Dense(37*4, ReLU) - Dense(64, ReLU) - Dense(4, Linear)\n",
    "\"\"\"\n",
    "def network():\n",
    "    act = torch.nn.ReLU\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(37*4, 64),\n",
    "                           act(),\n",
    "                           torch.nn.Linear(64, 64),\n",
    "                           act(),\n",
    "                           torch.nn.Linear(64, 4))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. Build the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-5\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "REPLAY_MEMORY_SIZE = 15000\n",
    "BATCH_SIZE = 64\n",
    "EPSILON_LOWER_BOUND = 0.008\n",
    "EPSILON_DECAY_RATIO = 0.90\n",
    "TOTAL_EPISODES = 4000\n",
    "OBSERVATION_STEP = 100\n",
    "MODEL_UPDATE_EPISODE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size=37, action_size=4, build_network=network(), lr=LEARNING_RATE,\n",
    "                discount_factor=DISCOUNT_FACTOR, replay_mem_size=REPLAY_MEMORY_SIZE, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64\\\\Banana.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]  # Refer to the code \"1. Start the Environment\" above.\n",
    "print('[INFO] Environment initialized.')\n",
    "\n",
    "scores = []\n",
    "steps = 0\n",
    "epsilon = 1\n",
    "epsilon_decay = (1 - EPSILON_LOWER_BOUND) / ((TOTAL_EPISODES * 300) * EPSILON_DECAY_RATIO)  # Calculate how much epsilon will reduce per step.\n",
    "avg_reward = collections.deque(maxlen=100)\n",
    "state_data = np.empty([37*4])\n",
    "for e in range(TOTAL_EPISODES):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    if e is 0:\n",
    "        state_data = np.stack([state, state, state, state]).flatten()  # fill observation data\n",
    "    score = 0\n",
    "    while True:\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, 4)  # random move\n",
    "        else:\n",
    "            action = agent.get_action(state_data)\n",
    "        env_info = env.step(int(action))[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        next_state = np.append(next_state, state_data[0:-37])\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        \n",
    "        agent.append_replay_memory(state_data, action, reward, next_state, done)\n",
    "        \n",
    "        if steps > OBSERVATION_STEP:\n",
    "            agent.train()\n",
    "            agent.update_target_network_smooth()\n",
    "            if epsilon > EPSILON_LOWER_BOUND:\n",
    "                epsilon -= epsilon_decay\n",
    "        score += reward\n",
    "        state_data = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            scores.append(score)\n",
    "            avg_reward.append(score)\n",
    "            break\n",
    "             \n",
    "    if e % 40 == 0:\n",
    "        max_q = agent.max_q(state_data)\n",
    "        if not e >= 100:\n",
    "            print('[Step ' + str(steps + 1).zfill(8) + '; Episode ' + str(e).zfill(6) + '] reward: ' + str(score),\n",
    "                  \"  max Q: \" + str(max_q) +\n",
    "                  \"  ε= \" + str(epsilon))\n",
    "        else:\n",
    "            print('[Step ' + str(steps + 1).zfill(8) + '; Episode ' + str(e).zfill(6) + '] reward: ' + str(score),\n",
    "                  \"  max Q: \" + str(max_q) +\n",
    "                  \"  ε= \" + str(epsilon) + \n",
    "                 \"  Avg. score: \" + str(np.average(avg_reward)))\n",
    "    \n",
    "   # if e % MODEL_UPDATE_EPISODE == 0:\n",
    "        #agent.update_target_network()\n",
    "    \n",
    "    if len(avg_reward) is 100:\n",
    "        avg = np.average(avg_reward)\n",
    "        if avg > 13:\n",
    "            print(\"[INFO] Training completed. Total episode: \" + str(e))\n",
    "            break\n",
    "\n",
    "with open('save\\\\scores.pickle', 'wb') as f:\n",
    "    pickle.dump(scores, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "agent.save_model('save\\\\model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-2. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size=37, action_size=4, build_network=network(), lr=LEARNING_RATE,\n",
    "                discount_factor=DISCOUNT_FACTOR, replay_mem_size=REPLAY_MEMORY_SIZE, batch_size=BATCH_SIZE)\n",
    "agent.restore_model('save\\\\model')\n",
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64\\\\Banana.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]  # Refer to the code \"1. Start the Environment\" above.\n",
    "print('[INFO] Environment initialized.')\n",
    "\n",
    "steps = 0\n",
    "avg_reward = collections.deque(maxlen=100)  # Scores from recent 100 episodes\n",
    "for e in range(100):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    score = 0\n",
    "    while True:  # until the episode ends\n",
    "        action = agent.get_action(state)\n",
    "        env_info = env.step(int(action))[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        \n",
    "        score += reward\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            avg_reward.append(score)\n",
    "            break\n",
    "             \n",
    "    if e % 20 == 0:\n",
    "            print('Episode ' + str(e).zfill(3) + ' reward: ' + str(score))\n",
    "\n",
    "print(\"Avg(100 episode) score: \" + str(np.average(avg_reward)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5. Visualising the training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(range(len(scores)), scores)\n",
    "ax.set(xlabel='Episode', ylabel=\"total score\")\n",
    "plt.title(\"Total Reward\")\n",
    "plt.savefig(\"save\\\\reward.png\", dpi=200)  # save the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'd like to solve this project with other reinforcement learning methods, such as Dueling DQN, Double DQN, DRQN, and so on.\n",
    "- I'd like to use DQN-based network in video game invironment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
